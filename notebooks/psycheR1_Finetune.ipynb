{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf362e79",
      "metadata": {},
      "source": [
        "# Psyche-R1: Baseline vs LoRA Fine-Tuned (CPU-safe)\n",
        "\n",
        "**Goal.** Show a simple, reproducible baseline vs fine-tuned comparison on a small mental-health counseling dataset (CounselChat), using a tiny instruct LLM (**Qwen2.5-0.5B-Instruct**) and **LoRA adapters** trained on **CPU**.\n",
        "\n",
        "**What we‚Äôll do**\n",
        "1) Load & clean `counselchat-data.csv`\n",
        "2) Build 80/10/10 train/dev/test splits\n",
        "3) Baseline: zero-shot generation (no training)\n",
        "4) LoRA fine-tune (CPU manual loop, small steps)\n",
        "5) Evaluate: generate with the adapter, compare side-by-side\n",
        "6) Export CSVs + quick empathy probe\n",
        "\n",
        "> Note: CPU training is slow and minimal. Before start ‚Äî Clone the repository and follow simple instructions from my github which the link can be find in 1st page of my report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ee0043",
      "metadata": {},
      "source": [
        "## Imports, Reproducibility, and Project Paths\n",
        "\n",
        "This cell sets up the **runtime environment** for the notebook.\n",
        "\n",
        "### What it does\n",
        "- **Core libraries:**  \n",
        "  - `os`, `pathlib.Path` for filesystem ops  \n",
        "  - `math`, `json`, `re`, `gc` for utilities and cleanup  \n",
        "  - `random`, `numpy`, `pandas` for data handling and reproducibility  \n",
        "  - `sklearn.model_selection` for potential future splits (explicit split later)  \n",
        "- **PyTorch & HF stack:**  \n",
        "  - `torch` for tensors and training  \n",
        "  - `datasets.Dataset` for wrapping Python lists ‚Üí HF datasets  \n",
        "  - `transformers` for tokenizer/model + training utilities  \n",
        "  - `peft` for LoRA adapters (parameter-efficient fine-tuning)\n",
        "\n",
        "### Reproducibility\n",
        "We **fix the random seed** across Python, NumPy, and PyTorch:\n",
        "```text\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os, math, json, re, random, gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer)\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DATA_CSV_PATH = \"../data/raw/counselchat-data.csv\"\n",
        "OUT_DIR = Path(\"runs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26047cd",
      "metadata": {},
      "source": [
        "## **Dataset Loading & Preprocessing**\n",
        "\n",
        "This section loads the **CounselChat** dataset and prepares it for fine-tuning.  \n",
        "Several important preprocessing steps are applied:\n",
        "\n",
        "### **‚úî HTML Cleaning**\n",
        "CounselChat answers contain `<br>`, `<p>`, and other tags.  \n",
        "We clean them using a custom `strip_html()` function that:\n",
        "- removes all HTML tags  \n",
        "- converts `<br>` into newlines  \n",
        "- trims whitespace  \n",
        "\n",
        "### **‚úî Input Construction**\n",
        "Each sample combines **questionTitle** and **questionText** into a single ‚Äúinput‚Äù."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     s \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, s)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(DATA_CSV_PATH)\n\u001b[0;32m     11\u001b[0m candidate_cat_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestioncategory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     12\u001b[0m cat_col \u001b[38;5;241m=\u001b[39m candidate_cat_cols[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m candidate_cat_cols \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "def strip_html(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = re.sub(r\"<br\\s*/?>\", \"\\n\", s, flags=re.I)\n",
        "    s = re.sub(r\"<[^>]+>\", \"\", s) \n",
        "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "df = pd.read_csv(DATA_CSV_PATH)\n",
        "\n",
        "candidate_cat_cols = [c for c in df.columns if c.lower() in {\"questioncategory\",\"category\",\"tags\",\"topics\"}]\n",
        "cat_col = candidate_cat_cols[0] if candidate_cat_cols else None\n",
        "\n",
        "df[\"clean_answer\"] = df[\"answerText\"].astype(str).apply(strip_html)\n",
        "\n",
        "def make_input(row):\n",
        "    title = str(row.get(\"questionTitle\",\"\")).strip()\n",
        "    body  = str(row.get(\"questionText\",\"\")).strip()\n",
        "    if title and body and title not in body:\n",
        "        return f\"{title}\\n\\n{body}\"\n",
        "    return body or title\n",
        "\n",
        "def make_instruction(row):\n",
        "    base = \"Provide an empathetic, evidence-based counseling reply.\"\n",
        "    if cat_col and pd.notna(row.get(cat_col)) and str(row.get(cat_col)).strip():\n",
        "        return f\"{base} Topic: {str(row.get(cat_col)).strip()}.\"\n",
        "    return base\n",
        "\n",
        "usable = df[(df[\"clean_answer\"].str.len() >= 20) & (df[\"questionText\"].astype(str).str.len() >= 20)].copy()\n",
        "\n",
        "records = []\n",
        "for _, r in usable.iterrows():\n",
        "    records.append({\n",
        "        \"instruction\": make_instruction(r),\n",
        "        \"input\": make_input(r),\n",
        "        \"output\": r[\"clean_answer\"]\n",
        "    })\n",
        "\n",
        "rng = np.random.RandomState(SEED)\n",
        "idx = np.arange(len(records))\n",
        "rng.shuffle(idx)\n",
        "n = len(idx); n_train = int(0.8*n); n_dev = int(0.1*n)\n",
        "train_idx = idx[:n_train]; dev_idx = idx[n_train:n_train+n_dev]; test_idx = idx[n_train+n_dev:]\n",
        "\n",
        "splits = {\n",
        "    \"train\": [records[i] for i in train_idx],\n",
        "    \"dev\":   [records[i] for i in dev_idx],\n",
        "    \"test\":  [records[i] for i in test_idx],\n",
        "}\n",
        "\n",
        "print({k: len(v) for k,v in splits.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420006dd",
      "metadata": {},
      "source": [
        "We use **Qwen/Qwen2.5-0.5B-Instruct** ‚Äî a ~0.5B parameter instruct model suitable for CPU demos.\n",
        "\n",
        "- MAX_LEN capped at 1024\n",
        "- Batch sizes are small (CPU-friendly)\n",
        "- We will do baseline generation with the base model (no training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  \n",
        "USE_4BIT = True\n",
        "USE_BF16 = True  \n",
        "MAX_LEN = 1024\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 16\n",
        "EPOCHS = 1     \n",
        "LR = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] 4-bit load failed, falling back to non-quantized FP16/BF16. Reason:\n",
            "        AttributeError(\"'frozenset' object has no attribute 'discard'\")\n",
            "Loaded model. 4-bit active: False | CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "USE_4BIT = True         \n",
        "USE_BF16 = True          \n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def load_base_model(model_name: str, use_4bit: bool, use_bf16: bool):\n",
        "    # tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    bnb_cfg = None\n",
        "    if use_4bit:\n",
        "        bnb_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "\n",
        "    dtype_val = torch.bfloat16 if (use_bf16 and torch.cuda.is_available()) else None\n",
        "\n",
        "    # FIX: dtype ‚Üí torch_dtype\n",
        "    def _create_model(_bnb_cfg):\n",
        "        return AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=_bnb_cfg,\n",
        "            torch_dtype=dtype_val   \n",
        "        )\n",
        "\n",
        "    try:\n",
        "        base_model = _create_model(bnb_cfg)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] 4-bit load failed, falling back to non-quantized FP16/BF16. Reason:\")\n",
        "        print(\"       \", repr(e))\n",
        "        base_model = _create_model(None)  \n",
        "        use_4bit = False\n",
        "\n",
        "    base_model.eval()\n",
        "    return tokenizer, base_model, use_4bit\n",
        "\n",
        "tokenizer, base_model, USE_4BIT = load_base_model(MODEL_NAME, USE_4BIT, USE_BF16)\n",
        "print(\"Loaded model. 4-bit active:\", USE_4BIT, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f2bb88",
      "metadata": {},
      "source": [
        "## Prompt Formatting + Baseline Response Generation\n",
        "\n",
        "This section defines **how prompts are constructed** and **how the baseline model generates answers** before any fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "##  `format_prompt()`  \n",
        "This function formats each example into an instruction-following prompt.\n",
        "\n",
        "```python\n",
        "def format_prompt(inst: str, inp: str) -> str:\n",
        "    if inp and inp.strip():\n",
        "        return f\"Instruction: {inst}\\nInput: {inp}\\nAnswer:\"\n",
        "    return f\"Instruction: {inst}\\nAnswer:\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(inst: str, inp: str) -> str:\n",
        "    if inp and inp.strip():\n",
        "        return f\"Instruction: {inst}\\nInput: {inp}\\nAnswer:\"\n",
        "    return f\"Instruction: {inst}\\nAnswer:\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_responses(model, examples, max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9):\n",
        "    outputs = []\n",
        "    for ex in examples:\n",
        "        prompt = format_prompt(ex[\"instruction\"], ex[\"input\"])\n",
        "        enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(model.device)\n",
        "        out = model.generate(**enc, max_new_tokens=max_new_tokens, do_sample=do_sample, temperature=temperature, top_p=top_p)\n",
        "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        pred = text[len(prompt):].strip()\n",
        "        outputs.append(pred)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4f62a7",
      "metadata": {},
      "source": [
        "## Baseline Output Generation (Before Fine-Tuning)\n",
        "\n",
        "This section generates the **baseline responses** from the untrained model (Qwen2.5-0.5B-Instruct).  \n",
        "These outputs serve as the *‚Äúbefore fine-tuning‚Äù* reference point.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Move Model to CPU\n",
        "```python\n",
        "base_model.to(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  if self.epsilon_cutoff is not None and self.epsilon_cutoff != 0.0:\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  minor_issues[\"eta_cutoff\"] = greedy_wrong_parameter_msg.format(\n",
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  )\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved baseline outputs to: runs\\baseline_outputs.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>reference</th>\n",
              "      <th>baseline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>Why do my boyfriend and I have such trouble co...</td>\n",
              "      <td>Try having a conversation with your boyfriend ...</td>\n",
              "      <td>Communication is key to any relationship. When...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>My new husband constantly talks to himself\\n\\n...</td>\n",
              "      <td>Some people simply talk to themselves as a way...</td>\n",
              "      <td>It's important to recognize that communication...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>How can I keep a long distance relationship go...</td>\n",
              "      <td>Hello. You are asking a very good question abo...</td>\n",
              "      <td>It is understandable that you are feeling anxi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         instruction  \\\n",
              "0  Provide an empathetic, evidence-based counseli...   \n",
              "1  Provide an empathetic, evidence-based counseli...   \n",
              "2  Provide an empathetic, evidence-based counseli...   \n",
              "\n",
              "                                               input  \\\n",
              "0  Why do my boyfriend and I have such trouble co...   \n",
              "1  My new husband constantly talks to himself\\n\\n...   \n",
              "2  How can I keep a long distance relationship go...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Try having a conversation with your boyfriend ...   \n",
              "1  Some people simply talk to themselves as a way...   \n",
              "2  Hello. You are asking a very good question abo...   \n",
              "\n",
              "                                            baseline  \n",
              "0  Communication is key to any relationship. When...  \n",
              "1  It's important to recognize that communication...  \n",
              "2  It is understandable that you are feeling anxi...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.to(\"cpu\")\n",
        "\n",
        "TEST_SAMPLES = 10 #10 rows\n",
        "test_subset = splits[\"test\"][:TEST_SAMPLES]\n",
        "\n",
        "baseline_outputs = generate_responses(base_model, test_subset, do_sample=False)\n",
        "\n",
        "baseline_df = pd.DataFrame({\n",
        "    \"instruction\": [ex[\"instruction\"] for ex in test_subset],\n",
        "    \"input\":       [ex[\"input\"] for ex in test_subset],\n",
        "    \"reference\":   [ex[\"output\"] for ex in test_subset],\n",
        "    \"baseline\":    baseline_outputs\n",
        "})\n",
        "\n",
        "baseline_csv = OUT_DIR / \"baseline_outputs.csv\"\n",
        "baseline_df.to_csv(baseline_csv, index=False)\n",
        "print(\"Saved baseline outputs to:\", baseline_csv)\n",
        "baseline_df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab76230",
      "metadata": {},
      "source": [
        "## Convert Records into HuggingFace Datasets + Tokenisation\n",
        "\n",
        "After we create the cleaned `records` list and the train/dev/test splits, we now need to convert them into a format suitable for supervised fine-tuning (SFT).  \n",
        "This step transforms human-readable text into **token IDs**, and prepares the dataset so the model can learn to predict counselor-style answers.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Convert Python Records ‚Üí HuggingFace Dataset\n",
        "We start by turning a normal list of Python dictionaries into a HuggingFace `Dataset` object:\n",
        "\n",
        "```python\n",
        "def build_hf_dataset(recs):\n",
        "    return Dataset.from_list(recs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0848d67c54cd4b9d9c2671e65876c9ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1102 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "006c1e76b4a04a16a843713a44ebcab5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/137 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(1102, 137)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_hf_dataset(recs):\n",
        "    return Dataset.from_list(recs)\n",
        "\n",
        "def tokenize_sft(rec):\n",
        "    inst, inp, out = rec[\"instruction\"], rec[\"input\"], rec[\"output\"]\n",
        "    prompt = format_prompt(inst, inp)\n",
        "    full = prompt + \" \" + out + tokenizer.eos_token\n",
        "    toks = tokenizer(full, max_length=MAX_LEN, truncation=True)\n",
        "    toks[\"labels\"] = toks[\"input_ids\"].copy()\n",
        "    return toks\n",
        "\n",
        "train_ds = build_hf_dataset(splits[\"train\"]).map(tokenize_sft, remove_columns=list(splits[\"train\"][0].keys()))\n",
        "dev_ds   = build_hf_dataset(splits[\"dev\"]).map(tokenize_sft,   remove_columns=list(splits[\"dev\"][0].keys()))\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "len(train_ds), len(dev_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9256b6da",
      "metadata": {},
      "source": [
        "## Attach LoRA Adapters (CPU-safe) and Verify Trainable Parameters\n",
        "\n",
        "This cell loads the base model **on CPU in FP32**, attaches **LoRA adapters** to attention/MLP blocks, switches the model to **train mode**, and verifies that only the LoRA layers are **trainable** (everything else stays frozen). This is what makes fine-tuning feasible on limited hardware.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 1) Load the base model on CPU (float32)\n",
        "\n",
        "We explicitly load the tokenizer and model on **CPU** with `torch_dtype=torch.float32`.  \n",
        "FP32 on CPU ensures autograd works reliably without GPU/quantization dependencies.\n",
        "\n",
        "Key points:\n",
        "- If the tokenizer has no `pad_token`, we reuse `eos_token` to avoid padding errors.\n",
        "- `model.eval()` here is just a safe default; we‚Äôll flip to `train()` after adding LoRA.\n",
        "\n",
        "What it enables:\n",
        "- Deterministic, hardware-agnostic setup.\n",
        "- No reliance on `bitsandbytes` or CUDA.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 2) Pick LoRA target modules automatically\n",
        "\n",
        "We scan `model.named_modules()` and match common projection names in **attention** (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and **MLP** (`up_proj`, `down_proj`, `gate_proj`, or equivalents like `c_attn`, `c_proj`, `w1/w2/w3`).\n",
        "\n",
        "Why:\n",
        "- These layers control most of the model‚Äôs expressive power in language generation.\n",
        "- LoRA on these layers gives strong adaptation with a tiny number of trainable parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 3) Define LoRA config and wrap the model\n",
        "\n",
        "We use:\n",
        "- `r=16`, `lora_alpha=32`, `lora_dropout=0.05`\n",
        "- `bias=\"none\"` (don‚Äôt touch original biases)\n",
        "- `task_type=\"CAUSAL_LM\"`\n",
        "\n",
        "Effect:\n",
        "- Only a **small adapter** is trained (~1.7% of total parameters), keeping the base weights frozen.\n",
        "- Memory and compute are minimized ‚Äî ideal for CPU fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 4) Switch to train mode and confirm what‚Äôs trainable\n",
        "\n",
        "- `model.train()` enables gradient flow through LoRA adapters.\n",
        "- We list parameters with `requires_grad=True` to confirm LoRA attached correctly.\n",
        "- Typical printout:\n",
        "  - **Total params** (count of named tensors, not total scalar count)\n",
        "  - **Trainable params** (only LoRA tensors)\n",
        "  - First few trainable tensor names (e.g., `...q_proj.lora_A/B...`)\n",
        "\n",
        "**What you should see**\n",
        "- Dozens/hundreds of LoRA tensors listed as trainable.\n",
        "- A summary like:  \n",
        "  `trainable params: 8,798,208 || all params: 502,830,976 || trainable%: ~1.75%`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Outcome of this cell\n",
        "\n",
        "- Base model: **on CPU, FP32** (safe and portable).\n",
        "- LoRA: **attached to attention/MLP** blocks.\n",
        "- Training mode: **enabled**.\n",
        "- Diagnostics: **printed** list of trainable LoRA parameters and a % summary.\n",
        "\n",
        "This sets up the model for the **manual CPU training loop** in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total params: 626\n",
            "Trainable params: 336\n",
            "First few trainable params:\n",
            "   base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
            "LoRA attached successfully. Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "FORCE_CPU = True\n",
        "USE_BF16 = False       \n",
        "USE_4BIT = False        \n",
        "\n",
        "def load_base_model_cpu_fp32(model_name: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cpu\",\n",
        "        torch_dtype=torch.float32   \n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, train_model = load_base_model_cpu_fp32(MODEL_NAME)\n",
        "\n",
        "def guess_lora_targets(m):\n",
        "    names = [n for n, _ in m.named_modules()]\n",
        "    keys = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"up_proj\", \"down_proj\", \"gate_proj\",\n",
        "        \"c_attn\", \"c_proj\",\n",
        "        \"w1\", \"w2\", \"w3\"\n",
        "    ]\n",
        "    return sorted({k for k in keys if any(k in n for n in names)})\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=guess_lora_targets(train_model),\n",
        ")\n",
        "\n",
        "train_model = get_peft_model(train_model, lora_cfg)\n",
        "\n",
        "train_model.train()\n",
        "train_model.to(\"cpu\")\n",
        "\n",
        "trainable_params = [(n, p) for n, p in train_model.named_parameters() if p.requires_grad]\n",
        "frozen_params = [(n, p) for n, p in train_model.named_parameters() if not p.requires_grad]\n",
        "\n",
        "print(f\"Total params: {len(trainable_params) + len(frozen_params)}\")\n",
        "print(f\"Trainable params: {len(trainable_params)}\")\n",
        "\n",
        "if not trainable_params:\n",
        "    raise RuntimeError(\"‚ùå No trainable parameters ‚Äî LoRA didn't attach.\")\n",
        "\n",
        "print(\"First few trainable params:\")\n",
        "for n, _ in trainable_params[:10]:\n",
        "    print(\"  \", n)\n",
        "\n",
        "train_model.print_trainable_parameters()\n",
        "print(\"LoRA attached successfully. Device:\", next(train_model.parameters()).device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aff6290",
      "metadata": {},
      "source": [
        "## Manual Fine-Tuning Loop on CPU (LoRA-only training)\n",
        "\n",
        "This cell performs **supervised fine-tuning** on CPU using a **custom PyTorch loop**. We only train the **LoRA adapter weights** (the base model stays frozen), which makes training feasible without a GPU.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment + Device\n",
        "- We disable external loggers (`WANDB_*` env vars) and **hide CUDA** (`CUDA_VISIBLE_DEVICES=\"\"`) to guarantee a **CPU-only** run, avoiding accidental GPU calls.\n",
        "- `device = torch.device(\"cpu\")` and `train_model.to(device)` move the LoRA-wrapped model to CPU.\n",
        "- `train_model.train()` enables dropout and gradient computation.\n",
        "\n",
        "---\n",
        "\n",
        "### Hyperparameters\n",
        "- `EPOCHS = 1` ‚Äî one pass over the training set for a quick demo.\n",
        "- `BATCH_SIZE = 1` ‚Äî keeps memory usage tiny on CPU.\n",
        "- `GRAD_ACCUM = 16` ‚Äî **gradient accumulation** simulates an effective batch of ~16 examples without increasing RAM.\n",
        "- `LR = 2e-4` ‚Äî learning rate for AdamW.\n",
        "- `MAX_STEPS = None` ‚Äî optional early stop (keep `None` for full epoch).\n",
        "\n",
        "---\n",
        "\n",
        "### DataLoader\n",
        "```python\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting manual fine-tune on CPU:\n",
            "Trainable tensors: 336\n",
            "Device: cpu\n",
            "Epochs: 1, Grad Accum: 16\n",
            "\n",
            "===== Epoch 1/1 =====\n",
            "[step 1] loss=2.6685\n",
            "[step 2] loss=2.8328\n",
            "[step 3] loss=2.4674\n",
            "[step 4] loss=2.5675\n",
            "[step 5] loss=2.6215\n",
            "[step 6] loss=2.3738\n",
            "[step 7] loss=2.6239\n",
            "[step 8] loss=2.7623\n",
            "[step 9] loss=2.5349\n",
            "[step 10] loss=3.0732\n",
            "[step 11] loss=2.4597\n",
            "[step 12] loss=2.7410\n",
            "[step 13] loss=2.0946\n",
            "[step 14] loss=1.9922\n",
            "[step 15] loss=1.9471\n",
            "[step 16] loss=2.4989\n",
            "[step 17] loss=2.6323\n",
            "[step 18] loss=2.2356\n",
            "[step 19] loss=2.1088\n",
            "[step 20] loss=2.4865\n",
            "[step 21] loss=2.2616\n",
            "[step 22] loss=2.6919\n",
            "[step 23] loss=2.2478\n",
            "[step 24] loss=2.1239\n",
            "[step 25] loss=2.4429\n",
            "[step 26] loss=3.3551\n",
            "[step 27] loss=2.0640\n",
            "[step 28] loss=2.1978\n",
            "[step 29] loss=2.3860\n",
            "[step 30] loss=2.3989\n",
            "[step 31] loss=2.0638\n",
            "[step 32] loss=2.2060\n",
            "[step 33] loss=2.5441\n",
            "[step 34] loss=2.2393\n",
            "[step 35] loss=3.0534\n",
            "[step 36] loss=2.3217\n",
            "[step 37] loss=2.7821\n",
            "[step 38] loss=2.5646\n",
            "[step 39] loss=2.2073\n",
            "[step 40] loss=2.1424\n",
            "[step 41] loss=2.4196\n",
            "[step 42] loss=2.4540\n",
            "[step 43] loss=2.3746\n",
            "[step 44] loss=2.2599\n",
            "[step 45] loss=1.9245\n",
            "[step 46] loss=2.5360\n",
            "[step 47] loss=1.4512\n",
            "[step 48] loss=2.1478\n",
            "[step 49] loss=2.3494\n",
            "[step 50] loss=2.0150\n",
            "[step 51] loss=2.6789\n",
            "[step 52] loss=2.4708\n",
            "[step 53] loss=2.1378\n",
            "[step 54] loss=2.1788\n",
            "[step 55] loss=2.2030\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     43\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     52\u001b[0m     (loss \u001b[38;5;241m/\u001b[39m GRAD_ACCUM)\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\peft\\peft_model.py:1850\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1849\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1850\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1851\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1852\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1853\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1854\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1855\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1856\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1857\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1858\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1859\u001b[0m         )\n\u001b[0;32m   1861\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:1164\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:895\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:623\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:501\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:771\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_input_dtype(x, lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_variant:  \u001b[38;5;66;03m# vanilla LoRA\u001b[39;00m\n\u001b[1;32m--> 771\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m lora_B(lora_A(dropout(x))) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_variant[active_adapter]\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    775\u001b[0m         active_adapter\u001b[38;5;241m=\u001b[39mactive_adapter,\n\u001b[0;32m    776\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    777\u001b[0m         result\u001b[38;5;241m=\u001b[39mresult,\n\u001b[0;32m    778\u001b[0m     )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"DISABLE_WANDB\"] = \"true\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "train_model.to(device)\n",
        "train_model.train()\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 16\n",
        "LR = 2e-4\n",
        "MAX_STEPS = None \n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n",
        "optim_params = [p for p in train_model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(optim_params, lr=LR)\n",
        "\n",
        "global_step = 0\n",
        "optimizer.zero_grad()\n",
        "\n",
        "print(\"Starting manual fine-tune on CPU:\")\n",
        "print(f\"Trainable tensors: {len(optim_params)}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Epochs: {EPOCHS}, Grad Accum: {GRAD_ACCUM}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = train_model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"],\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        (loss / GRAD_ACCUM).backward()\n",
        "\n",
        "        if (batch_idx + 1) % GRAD_ACCUM == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(optim_params, max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            print(f\"[step {global_step}] loss={loss.item():.4f}\")\n",
        "\n",
        "            if MAX_STEPS is not None and global_step >= MAX_STEPS:\n",
        "                break\n",
        "\n",
        "    if MAX_STEPS is not None and global_step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "if (batch_idx + 1) % GRAD_ACCUM != 0:\n",
        "    torch.nn.utils.clip_grad_norm_(optim_params, max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    global_step += 1\n",
        "    print(f\"[final step {global_step}] (last partial batch)\")\n",
        "\n",
        "print(\"\\nFinished CPU fine-tuning loop.\")\n",
        "\n",
        "adapter_out = OUT_DIR / \"psyche_r1_sft\"\n",
        "adapter_out.mkdir(parents=True, exist_ok=True)\n",
        "train_model.save_pretrained(str(adapter_out))\n",
        "tokenizer.save_pretrained(str(adapter_out))\n",
        "print(f\"Saved fine-tuned LoRA adapter to: {adapter_out}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40fd3cb",
      "metadata": {},
      "source": [
        "## Reload Fine-Tuned Adapters + Compare Against Baseline\n",
        "\n",
        "This cell **reloads the base model on CPU**, **attaches the saved LoRA adapters**, generates outputs on the same test subset, and writes a **side-by-side CSV** (`baseline_vs_finetuned.csv`) for inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at 'runs\\psyche_r1_sft'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\peft\\config.py:262\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[1;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    263\u001b[0m         model_id,\n\u001b[0;32m    264\u001b[0m         CONFIG_NAME,\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The name cannot start or end with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and the maximum length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
            "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'runs\\psyche_r1_sft'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m      6\u001b[0m ft_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      7\u001b[0m     MODEL_NAME,\n\u001b[0;32m      8\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,          \n\u001b[0;32m      9\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32  \n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m ft_model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpsyche_r1_sft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ft_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     14\u001b[0m ft_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\peft\\peft_model.py:440\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[1;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[1;32m--> 440\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[0;32m    450\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
            "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\peft\\config.py:268\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[1;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    263\u001b[0m             model_id,\n\u001b[0;32m    264\u001b[0m             CONFIG_NAME,\n\u001b[0;32m    265\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "\u001b[1;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'runs\\psyche_r1_sft'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "ft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cpu\",          \n",
        "    torch_dtype=torch.float32  \n",
        ")\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(ft_model, str(OUT_DIR / \"psyche_r1_sft\"))\n",
        "ft_model.eval()\n",
        "ft_model.to(\"cpu\")\n",
        "\n",
        "print(\"Fine-tuned LoRA adapter loaded and ready for evaluation\")\n",
        "\n",
        "ft_outputs = generate_responses(\n",
        "    ft_model,\n",
        "    test_subset,\n",
        "    do_sample=False  \n",
        ")\n",
        "\n",
        "compare_df = pd.DataFrame({\n",
        "    \"instruction\": [ex[\"instruction\"] for ex in test_subset],\n",
        "    \"input\":       [ex[\"input\"] for ex in test_subset],\n",
        "    \"reference\":   [ex[\"output\"] for ex in test_subset],\n",
        "    \"baseline\":    baseline_df[\"baseline\"],\n",
        "    \"finetuned\":   ft_outputs\n",
        "})\n",
        "\n",
        "cmp_csv = OUT_DIR / \"baseline_vs_finetuned.csv\"\n",
        "compare_df.to_csv(cmp_csv, index=False)\n",
        "\n",
        "print(\" Saved comparison to:\", cmp_csv)\n",
        "display(compare_df.head(3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "psyche-r1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
