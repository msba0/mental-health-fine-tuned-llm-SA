{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf362e79",
      "metadata": {},
      "source": [
        "# Psyche-R1: Baseline vs LoRA Fine-Tuned (CPU-safe)\n",
        "\n",
        "**Goal.** Show a simple, reproducible baseline vs fine-tuned comparison on a small mental-health counseling dataset (CounselChat), using a tiny instruct LLM (**Qwen2.5-0.5B-Instruct**) and **LoRA adapters** trained on **CPU**.\n",
        "\n",
        "**What weâ€™ll do**\n",
        "1) Load & clean `counselchat-data.csv`\n",
        "2) Build 80/10/10 train/dev/test splits\n",
        "3) Baseline: zero-shot generation (no training)\n",
        "4) LoRA fine-tune (CPU manual loop, small steps)\n",
        "5) Evaluate: generate with the adapter, compare side-by-side\n",
        "6) Export CSVs + quick empathy probe\n",
        "\n",
        "> Note: CPU training is slow and minimal. This is for demonstration/assessment, not SOTA results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n\u001b[32m     10\u001b[39m                           DataCollatorForLanguageModeling, TrainingArguments, Trainer)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\u001b[32m     13\u001b[39m SEED = \u001b[32m42\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\trainer.py:42\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     43\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_hub_utils\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:60\u001b[39m\n\u001b[32m     57\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
          ]
        }
      ],
      "source": [
        "import os, math, json, re, random, gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer)\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DATA_CSV_PATH = \"../data/raw/counselchat-data.csv\"\n",
        "OUT_DIR = Path(\"runs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8acd576",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': 1102, 'dev': 137, 'test': 139}\n"
          ]
        }
      ],
      "source": [
        "def strip_html(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = re.sub(r\"<br\\s*/?>\", \"\\n\", s, flags=re.I)\n",
        "    s = re.sub(r\"<[^>]+>\", \"\", s) \n",
        "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "df = pd.read_csv(DATA_CSV_PATH)\n",
        "\n",
        "candidate_cat_cols = [c for c in df.columns if c.lower() in {\"questioncategory\",\"category\",\"tags\",\"topics\"}]\n",
        "cat_col = candidate_cat_cols[0] if candidate_cat_cols else None\n",
        "\n",
        "df[\"clean_answer\"] = df[\"answerText\"].astype(str).apply(strip_html)\n",
        "\n",
        "def make_input(row):\n",
        "    title = str(row.get(\"questionTitle\",\"\")).strip()\n",
        "    body  = str(row.get(\"questionText\",\"\")).strip()\n",
        "    if title and body and title not in body:\n",
        "        return f\"{title}\\n\\n{body}\"\n",
        "    return body or title\n",
        "\n",
        "def make_instruction(row):\n",
        "    base = \"Provide an empathetic, evidence-based counseling reply.\"\n",
        "    if cat_col and pd.notna(row.get(cat_col)) and str(row.get(cat_col)).strip():\n",
        "        return f\"{base} Topic: {str(row.get(cat_col)).strip()}.\"\n",
        "    return base\n",
        "\n",
        "usable = df[(df[\"clean_answer\"].str.len() >= 20) & (df[\"questionText\"].astype(str).str.len() >= 20)].copy()\n",
        "\n",
        "records = []\n",
        "for _, r in usable.iterrows():\n",
        "    records.append({\n",
        "        \"instruction\": make_instruction(r),\n",
        "        \"input\": make_input(r),\n",
        "        \"output\": r[\"clean_answer\"]\n",
        "    })\n",
        "\n",
        "rng = np.random.RandomState(SEED)\n",
        "idx = np.arange(len(records))\n",
        "rng.shuffle(idx)\n",
        "n = len(idx); n_train = int(0.8*n); n_dev = int(0.1*n)\n",
        "train_idx = idx[:n_train]; dev_idx = idx[n_train:n_train+n_dev]; test_idx = idx[n_train+n_dev:]\n",
        "\n",
        "splits = {\n",
        "    \"train\": [records[i] for i in train_idx],\n",
        "    \"dev\":   [records[i] for i in dev_idx],\n",
        "    \"test\":  [records[i] for i in test_idx],\n",
        "}\n",
        "\n",
        "print({k: len(v) for k,v in splits.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420006dd",
      "metadata": {},
      "source": [
        "We use **Qwen/Qwen2.5-0.5B-Instruct** â€” a ~0.5B parameter instruct model suitable for CPU demos.\n",
        "\n",
        "- MAX_LEN capped at 1024\n",
        "- Batch sizes are small (CPU-friendly)\n",
        "- We will do baseline generation with the base model (no training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  \n",
        "USE_4BIT = True\n",
        "USE_BF16 = True  \n",
        "MAX_LEN = 1024\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 16\n",
        "EPOCHS = 1     \n",
        "LR = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da264756",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (214696030.py, line 1)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "#pip install --upgrade bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
            "NVIDIA GeForce RTX 5060 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
            "If you want to use the NVIDIA GeForce RTX 5060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] 4-bit load failed, falling back to non-quantized FP16/BF16. Reason:\n",
            "        RuntimeError('CUDA error: no kernel image is available for execution on the device\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n')\n",
            "Loaded model. 4-bit active: False | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "USE_4BIT = True         \n",
        "USE_BF16 = True          \n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def load_base_model(model_name: str, use_4bit: bool, use_bf16: bool):\n",
        "    # tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    bnb_cfg = None\n",
        "    if use_4bit:\n",
        "        bnb_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "\n",
        "    dtype_val = torch.bfloat16 if (use_bf16 and torch.cuda.is_available()) else None\n",
        "\n",
        "    def _create_model(_bnb_cfg):\n",
        "        return AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=_bnb_cfg,\n",
        "            dtype=dtype_val\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        base_model = _create_model(bnb_cfg)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] 4-bit load failed, falling back to non-quantized FP16/BF16. Reason:\")\n",
        "        print(\"       \", repr(e))\n",
        "        base_model = _create_model(None) \n",
        "        use_4bit = False\n",
        "\n",
        "    base_model.eval()\n",
        "    return tokenizer, base_model, use_4bit\n",
        "\n",
        "tokenizer, base_model, USE_4BIT = load_base_model(MODEL_NAME, USE_4BIT, USE_BF16)\n",
        "print(\"Loaded model. 4-bit active:\", USE_4BIT, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(inst: str, inp: str) -> str:\n",
        "    if inp and inp.strip():\n",
        "        return f\"Instruction: {inst}\\nInput: {inp}\\nAnswer:\"\n",
        "    return f\"Instruction: {inst}\\nAnswer:\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_responses(model, examples, max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9):\n",
        "    outputs = []\n",
        "    for ex in examples:\n",
        "        prompt = format_prompt(ex[\"instruction\"], ex[\"input\"])\n",
        "        enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(model.device)\n",
        "        out = model.generate(**enc, max_new_tokens=max_new_tokens, do_sample=do_sample, temperature=temperature, top_p=top_p)\n",
        "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        pred = text[len(prompt):].strip()\n",
        "        outputs.append(pred)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved baseline outputs to: runs\\baseline_outputs.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>reference</th>\n",
              "      <th>baseline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>Why do my boyfriend and I have such trouble co...</td>\n",
              "      <td>Try having a conversation with your boyfriend ...</td>\n",
              "      <td>Communication is key to any relationship. When...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>My new husband constantly talks to himself\\n\\n...</td>\n",
              "      <td>Some people simply talk to themselves as a way...</td>\n",
              "      <td>It's important to recognize that communication...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>How can I keep a long distance relationship go...</td>\n",
              "      <td>Hello. You are asking a very good question abo...</td>\n",
              "      <td>It is understandable that you are feeling anxi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         instruction  \\\n",
              "0  Provide an empathetic, evidence-based counseli...   \n",
              "1  Provide an empathetic, evidence-based counseli...   \n",
              "2  Provide an empathetic, evidence-based counseli...   \n",
              "\n",
              "                                               input  \\\n",
              "0  Why do my boyfriend and I have such trouble co...   \n",
              "1  My new husband constantly talks to himself\\n\\n...   \n",
              "2  How can I keep a long distance relationship go...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Try having a conversation with your boyfriend ...   \n",
              "1  Some people simply talk to themselves as a way...   \n",
              "2  Hello. You are asking a very good question abo...   \n",
              "\n",
              "                                            baseline  \n",
              "0  Communication is key to any relationship. When...  \n",
              "1  It's important to recognize that communication...  \n",
              "2  It is understandable that you are feeling anxi...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.to(\"cpu\")\n",
        "\n",
        "TEST_SAMPLES = 10 #10 rows\n",
        "test_subset = splits[\"test\"][:TEST_SAMPLES]\n",
        "\n",
        "baseline_outputs = generate_responses(base_model, test_subset, do_sample=False)\n",
        "\n",
        "baseline_df = pd.DataFrame({\n",
        "    \"instruction\": [ex[\"instruction\"] for ex in test_subset],\n",
        "    \"input\":       [ex[\"input\"] for ex in test_subset],\n",
        "    \"reference\":   [ex[\"output\"] for ex in test_subset],\n",
        "    \"baseline\":    baseline_outputs\n",
        "})\n",
        "\n",
        "baseline_csv = OUT_DIR / \"baseline_outputs.csv\"\n",
        "baseline_df.to_csv(baseline_csv, index=False)\n",
        "print(\"Saved baseline outputs to:\", baseline_csv)\n",
        "baseline_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1102/1102 [00:00<00:00, 1304.78 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 137/137 [00:00<00:00, 1313.65 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1102, 137)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_hf_dataset(recs):\n",
        "    return Dataset.from_list(recs)\n",
        "\n",
        "def tokenize_sft(rec):\n",
        "    inst, inp, out = rec[\"instruction\"], rec[\"input\"], rec[\"output\"]\n",
        "    prompt = format_prompt(inst, inp)\n",
        "    full = prompt + \" \" + out + tokenizer.eos_token\n",
        "    toks = tokenizer(full, max_length=MAX_LEN, truncation=True)\n",
        "    toks[\"labels\"] = toks[\"input_ids\"].copy()\n",
        "    return toks\n",
        "\n",
        "train_ds = build_hf_dataset(splits[\"train\"]).map(tokenize_sft, remove_columns=list(splits[\"train\"][0].keys()))\n",
        "dev_ds   = build_hf_dataset(splits[\"dev\"]).map(tokenize_sft,   remove_columns=list(splits[\"dev\"][0].keys()))\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "len(train_ds), len(dev_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total params: 626\n",
            "Trainable params: 336\n",
            "âœ… First few trainable params:\n",
            "   base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "   base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "   base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
            "LoRA attached successfully. Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "FORCE_CPU = True\n",
        "USE_BF16 = False       \n",
        "USE_4BIT = False        \n",
        "\n",
        "def load_base_model_cpu_fp32(model_name: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cpu\",\n",
        "        torch_dtype=torch.float32   \n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, train_model = load_base_model_cpu_fp32(MODEL_NAME)\n",
        "\n",
        "def guess_lora_targets(m):\n",
        "    names = [n for n, _ in m.named_modules()]\n",
        "    keys = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"up_proj\", \"down_proj\", \"gate_proj\",\n",
        "        \"c_attn\", \"c_proj\",\n",
        "        \"w1\", \"w2\", \"w3\"\n",
        "    ]\n",
        "    return sorted({k for k in keys if any(k in n for n in names)})\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=guess_lora_targets(train_model),\n",
        ")\n",
        "\n",
        "train_model = get_peft_model(train_model, lora_cfg)\n",
        "\n",
        "train_model.train()\n",
        "train_model.to(\"cpu\")\n",
        "\n",
        "trainable_params = [(n, p) for n, p in train_model.named_parameters() if p.requires_grad]\n",
        "frozen_params = [(n, p) for n, p in train_model.named_parameters() if not p.requires_grad]\n",
        "\n",
        "print(f\"Total params: {len(trainable_params) + len(frozen_params)}\")\n",
        "print(f\"Trainable params: {len(trainable_params)}\")\n",
        "\n",
        "if not trainable_params:\n",
        "    raise RuntimeError(\"âŒ No trainable parameters â€” LoRA didn't attach.\")\n",
        "\n",
        "print(\"First few trainable params:\")\n",
        "for n, _ in trainable_params[:10]:\n",
        "    print(\"  \", n)\n",
        "\n",
        "train_model.print_trainable_parameters()\n",
        "print(\"LoRA attached successfully. Device:\", next(train_model.parameters()).device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting manual fine-tune on CPU...\n",
            "Trainable tensors: 336\n",
            "Device: cpu\n",
            "Epochs: 1, Grad Accum: 16\n",
            "\n",
            "===== Epoch 1/1 =====\n",
            "[step 1] loss=2.9945\n",
            "[step 2] loss=2.9017\n",
            "[step 3] loss=2.7857\n",
            "[step 4] loss=3.2014\n",
            "[step 5] loss=2.5689\n",
            "[step 6] loss=2.9188\n",
            "[step 7] loss=2.3194\n",
            "[step 8] loss=2.6603\n",
            "[step 9] loss=2.7417\n",
            "[step 10] loss=2.4911\n",
            "[step 11] loss=2.4491\n",
            "[step 12] loss=2.9548\n",
            "[step 13] loss=2.6540\n",
            "[step 14] loss=2.5123\n",
            "[step 15] loss=2.7482\n",
            "[step 16] loss=2.3909\n",
            "[step 17] loss=2.1442\n",
            "[step 18] loss=2.3488\n",
            "[step 19] loss=2.3313\n",
            "[step 20] loss=3.1156\n",
            "[step 21] loss=2.6760\n",
            "[step 22] loss=2.3375\n",
            "[step 23] loss=2.5947\n",
            "[step 24] loss=2.4627\n",
            "[step 25] loss=2.5901\n",
            "[step 26] loss=2.4313\n",
            "[step 27] loss=2.6478\n",
            "[step 28] loss=2.5955\n",
            "[step 29] loss=1.7774\n",
            "[step 30] loss=2.2323\n",
            "[step 31] loss=2.3256\n",
            "[step 32] loss=2.4782\n",
            "[step 33] loss=2.3496\n",
            "[step 34] loss=1.7259\n",
            "[step 35] loss=2.5157\n",
            "[step 36] loss=2.2343\n",
            "[step 37] loss=2.9321\n",
            "[step 38] loss=2.1441\n",
            "[step 39] loss=2.1475\n",
            "[step 40] loss=2.2275\n",
            "[step 41] loss=2.6385\n",
            "[step 42] loss=3.0284\n",
            "[step 43] loss=2.6322\n",
            "[step 44] loss=2.9724\n",
            "[step 45] loss=2.1311\n",
            "[step 46] loss=2.9915\n",
            "[step 47] loss=2.5019\n",
            "[step 48] loss=1.9532\n",
            "[step 49] loss=2.1276\n",
            "[step 50] loss=2.5925\n",
            "[step 51] loss=1.9758\n",
            "[step 52] loss=2.3646\n",
            "[step 53] loss=1.6982\n",
            "[step 54] loss=2.3409\n",
            "[step 55] loss=2.4123\n",
            "[step 56] loss=2.2832\n",
            "[step 57] loss=2.4461\n",
            "[step 58] loss=2.3688\n",
            "[step 59] loss=2.4167\n",
            "[step 60] loss=2.5557\n",
            "[step 61] loss=2.4817\n",
            "[step 62] loss=2.3189\n",
            "[step 63] loss=2.6616\n",
            "[step 64] loss=2.2390\n",
            "[step 65] loss=2.9040\n",
            "[step 66] loss=1.7181\n",
            "[step 67] loss=2.6939\n",
            "[step 68] loss=2.6430\n",
            "[final step 69] (last partial batch)\n",
            "\n",
            "âœ… Finished CPU fine-tuning loop.\n",
            "ðŸ’¾ Saved fine-tuned LoRA adapter to: runs\\psyche_r1_sft\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"DISABLE_WANDB\"] = \"true\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "train_model.to(device)\n",
        "train_model.train()\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 16\n",
        "LR = 2e-4\n",
        "MAX_STEPS = None \n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n",
        "optim_params = [p for p in train_model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(optim_params, lr=LR)\n",
        "\n",
        "global_step = 0\n",
        "optimizer.zero_grad()\n",
        "\n",
        "print(\"Starting manual fine-tune on CPU:\")\n",
        "print(f\"Trainable tensors: {len(optim_params)}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Epochs: {EPOCHS}, Grad Accum: {GRAD_ACCUM}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = train_model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"],\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        (loss / GRAD_ACCUM).backward()\n",
        "\n",
        "        if (batch_idx + 1) % GRAD_ACCUM == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(optim_params, max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            print(f\"[step {global_step}] loss={loss.item():.4f}\")\n",
        "\n",
        "            if MAX_STEPS is not None and global_step >= MAX_STEPS:\n",
        "                break\n",
        "\n",
        "    if MAX_STEPS is not None and global_step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "if (batch_idx + 1) % GRAD_ACCUM != 0:\n",
        "    torch.nn.utils.clip_grad_norm_(optim_params, max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    global_step += 1\n",
        "    print(f\"[final step {global_step}] (last partial batch)\")\n",
        "\n",
        "print(\"\\nFinished CPU fine-tuning loop.\")\n",
        "\n",
        "adapter_out = OUT_DIR / \"psyche_r1_sft\"\n",
        "adapter_out.mkdir(parents=True, exist_ok=True)\n",
        "train_model.save_pretrained(str(adapter_out))\n",
        "tokenizer.save_pretrained(str(adapter_out))\n",
        "print(f\"Saved fine-tuned LoRA adapter to: {adapter_out}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fine-tuned LoRA adapter loaded and ready for evaluation\n",
            "ðŸ’¾ Saved comparison to: runs\\baseline_vs_finetuned.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>reference</th>\n",
              "      <th>baseline</th>\n",
              "      <th>finetuned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>Why do my boyfriend and I have such trouble co...</td>\n",
              "      <td>Try having a conversation with your boyfriend ...</td>\n",
              "      <td>Communication is key to any relationship. When...</td>\n",
              "      <td>Hi, &amp;nbsp;I'm so glad you're able to talk with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>My new husband constantly talks to himself\\n\\n...</td>\n",
              "      <td>Some people simply talk to themselves as a way...</td>\n",
              "      <td>It's important to recognize that communication...</td>\n",
              "      <td>It sounds like you are experiencing some anxie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Provide an empathetic, evidence-based counseli...</td>\n",
              "      <td>How can I keep a long distance relationship go...</td>\n",
              "      <td>Hello. You are asking a very good question abo...</td>\n",
              "      <td>It is understandable that you are feeling anxi...</td>\n",
              "      <td>It sounds like you are in a difficult situatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         instruction  \\\n",
              "0  Provide an empathetic, evidence-based counseli...   \n",
              "1  Provide an empathetic, evidence-based counseli...   \n",
              "2  Provide an empathetic, evidence-based counseli...   \n",
              "\n",
              "                                               input  \\\n",
              "0  Why do my boyfriend and I have such trouble co...   \n",
              "1  My new husband constantly talks to himself\\n\\n...   \n",
              "2  How can I keep a long distance relationship go...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Try having a conversation with your boyfriend ...   \n",
              "1  Some people simply talk to themselves as a way...   \n",
              "2  Hello. You are asking a very good question abo...   \n",
              "\n",
              "                                            baseline  \\\n",
              "0  Communication is key to any relationship. When...   \n",
              "1  It's important to recognize that communication...   \n",
              "2  It is understandable that you are feeling anxi...   \n",
              "\n",
              "                                           finetuned  \n",
              "0  Hi, &nbsp;I'm so glad you're able to talk with...  \n",
              "1  It sounds like you are experiencing some anxie...  \n",
              "2  It sounds like you are in a difficult situatio...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "ft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cpu\",          \n",
        "    torch_dtype=torch.float32  \n",
        ")\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(ft_model, str(OUT_DIR / \"psyche_r1_sft\"))\n",
        "ft_model.eval()\n",
        "ft_model.to(\"cpu\")\n",
        "\n",
        "print(\"Fine-tuned LoRA adapter loaded and ready for evaluation\")\n",
        "\n",
        "ft_outputs = generate_responses(\n",
        "    ft_model,\n",
        "    test_subset,\n",
        "    do_sample=False  \n",
        ")\n",
        "\n",
        "compare_df = pd.DataFrame({\n",
        "    \"instruction\": [ex[\"instruction\"] for ex in test_subset],\n",
        "    \"input\":       [ex[\"input\"] for ex in test_subset],\n",
        "    \"reference\":   [ex[\"output\"] for ex in test_subset],\n",
        "    \"baseline\":    baseline_df[\"baseline\"],\n",
        "    \"finetuned\":   ft_outputs\n",
        "})\n",
        "\n",
        "cmp_csv = OUT_DIR / \"baseline_vs_finetuned.csv\"\n",
        "compare_df.to_csv(cmp_csv, index=False)\n",
        "\n",
        "print(\"ðŸ’¾ Saved comparison to:\", cmp_csv)\n",
        "display(compare_df.head(3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (psyche-r1)",
      "language": "python",
      "name": "psyche-r1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
