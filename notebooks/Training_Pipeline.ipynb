{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90365a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root in sys.path? True\n",
      "src exists? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "ROOT = Path(\"..\").resolve()         \n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# sanity check\n",
    "import importlib.util, os\n",
    "print(\"Repo root in sys.path?\", str(ROOT) in sys.path)\n",
    "print(\"src exists?\", (ROOT / \"src\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf982131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\User\\OneDrive\\Desktop\\(ZA4309) Emerging Technologies in Intelligence\\Project\\Fine-tune project\\mental-health-fine-tuned-llm-SA \n",
      "Has src?  True\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "ROOT = pathlib.Path(\"..\").resolve()\n",
    "os.chdir(ROOT)\n",
    "print(\"CWD:\", os.getcwd(), \"\\nHas src? \", (ROOT/\"src\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6adf90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088518b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: Qwen/Qwen2.5-0.5B-Instruct\n",
      "output_dir: runs/sft-qwen\n",
      "train_file: data/processed/counselchat_train.jsonl\n",
      "eval_file:  data/processed/counselchat_val.jsonl\n",
      "\n",
      "per_device_train_batch_size: 2\n",
      "gradient_accumulation_steps: 16\n",
      "num_train_epochs: 1\n",
      "learning_rate: 0.00001 \n",
      "logging_steps: 20\n",
      "save_steps: 200\n",
      "bf16: true\n",
      "report_to: tensorboard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(Path(\"configs/sft_qwen.yml\").read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7bc838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA detected but not usable, falling back to CPU: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "== Run plan ==\n",
      "Model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Train file: data/processed/counselchat_train.jsonl\n",
      "Eval file: data/processed/counselchat_val.jsonl\n",
      "Device: cpu bf16: False\n",
      "{'loss': 2.6949, 'grad_norm': 4.656899452209473, 'learning_rate': 1.3636363636363636e-06, 'epoch': 0.91}\n",
      "{'train_runtime': 2655.4327, 'train_samples_per_second': 0.263, 'train_steps_per_second': 0.008, 'train_loss': 2.6939523870294746, 'epoch': 1.0}\n",
      "Done. Checkpoints in: runs/sft-qwen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\psyche-r1\\lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\OneDrive\\Desktop\\(ZA4309) Emerging Technologies in Intelligence\\Project\\Fine-tune project\\mental-health-fine-tuned-llm-SA\\src\\train_sft.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]\n",
      "  5%|▍         | 1/22 [01:53<39:52, 113.92s/it]\n",
      "  9%|▉         | 2/22 [03:35<35:27, 106.37s/it]\n",
      " 14%|█▎        | 3/22 [05:38<36:11, 114.27s/it]\n",
      " 18%|█▊        | 4/22 [07:44<35:41, 119.00s/it]\n",
      " 23%|██▎       | 5/22 [09:47<34:05, 120.30s/it]\n",
      " 27%|██▋       | 6/22 [11:46<31:57, 119.86s/it]\n",
      " 32%|███▏      | 7/22 [13:35<29:04, 116.30s/it]\n",
      " 36%|███▋      | 8/22 [15:41<27:52, 119.48s/it]\n",
      " 41%|████      | 9/22 [17:43<26:04, 120.33s/it]\n",
      " 45%|████▌     | 10/22 [19:52<24:34, 122.88s/it]\n",
      " 50%|█████     | 11/22 [21:41<21:43, 118.50s/it]\n",
      " 55%|█████▍    | 12/22 [23:44<20:00, 120.01s/it]\n",
      " 59%|█████▉    | 13/22 [25:48<18:11, 121.23s/it]\n",
      " 64%|██████▎   | 14/22 [27:49<16:09, 121.24s/it]\n",
      " 68%|██████▊   | 15/22 [29:57<14:21, 123.07s/it]\n",
      " 73%|███████▎  | 16/22 [31:56<12:11, 121.85s/it]\n",
      " 77%|███████▋  | 17/22 [34:05<10:20, 124.05s/it]\n",
      " 82%|████████▏ | 18/22 [36:20<08:29, 127.25s/it]\n",
      " 86%|████████▋ | 19/22 [38:18<06:13, 124.57s/it]\n",
      " 91%|█████████ | 20/22 [40:26<04:10, 125.49s/it]\n",
      "                                                \n",
      "\n",
      " 91%|█████████ | 20/22 [40:26<04:10, 125.49s/it]\n",
      " 95%|█████████▌| 21/22 [42:24<02:03, 123.23s/it]\n",
      "100%|██████████| 22/22 [44:09<00:00, 118.01s/it]\n",
      "                                                \n",
      "\n",
      "100%|██████████| 22/22 [44:15<00:00, 118.01s/it]\n",
      "100%|██████████| 22/22 [44:15<00:00, 120.70s/it]\n"
     ]
    }
   ],
   "source": [
    "!python -m src.train_sft --config configs/sft_qwen.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f69cc1",
   "metadata": {},
   "source": [
    "## 📊 View TensorBoard (run in terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30caf8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open TensorBoard in a terminal:\n",
      "  tensorboard --logdir runs --port 6006\n"
     ]
    }
   ],
   "source": [
    "print(\"Open TensorBoard in a terminal:\\n  tensorboard --logdir runs --port 6006\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (psyche-R1)",
   "language": "python",
   "name": "psyche-r1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
